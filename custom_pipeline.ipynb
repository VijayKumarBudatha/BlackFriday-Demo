{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project gcp-ml-specialization-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud auth login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-aiplatform kfp google_cloud_pipeline_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=%env PATH\n",
    "%env PATH={PATH}:/home/jupyter/.local/bin\n",
    "REGION=\"us-central1\" # you can specify any other location of your choice\n",
    "\n",
    "# Get projet name\n",
    "shell_output=!gcloud config get-value project 2> /dev/null\n",
    "PROJECT_ID=shell_output[0]\n",
    "\n",
    "# Set bucket name\n",
    "BUCKET_NAME=\"gs://\"+PROJECT_ID+\"-black-friday-sales\"\n",
    "\n",
    "# Create bucket\n",
    "PIPELINE_ROOT = f\"{BUCKET_NAME}/pipeline_root_blackfridaysales/\"\n",
    "PIPELINE_ROOT\n",
    "\n",
    "SERVICE_ACCOUNT = \"296237320026-compute@developer.gserviceaccount.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple\n",
    "import typing\n",
    "from kfp import dsl\n",
    "from kfp.dsl import (Artifact,\n",
    "                        Dataset,\n",
    "                        Input,\n",
    "                        Model,\n",
    "                        Output,\n",
    "                        Metrics,\n",
    "                        ClassificationMetrics,\n",
    "                        component,\n",
    "                        OutputPath,\n",
    "                        InputPath)\n",
    "\n",
    "from kfp import compiler\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud.aiplatform import pipeline_jobs\n",
    "# from google_cloud_pipeline_components import aiplatform as gcc_aip\n",
    "from google_cloud_pipeline_components.v1.custom_job import utils\n",
    "import kfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"pandas\", \"pyarrow\", \"scikit-learn==1.3.0\"],\n",
    "  base_image=\"python:3.10\",\n",
    "  output_component_file=\"get_blackfriday_data.yaml\"\n",
    ")\n",
    "def get_blackfriday_data(\n",
    "  # An input parameter of type str.\n",
    "  url: str,\n",
    "  # Use Output[T] to get a metadata-rich handle to the output artifact of type `Dataset`.\n",
    "  # the artifact already has path in the place, where we run the pipeline\n",
    "  dataset_train: Output[Dataset],\n",
    "  dataset_test: Output[Dataset]\n",
    "):\n",
    "  import numpy as np\n",
    "  import pandas as pd\n",
    "  from sklearn.model_selection import train_test_split\n",
    "\n",
    "  df_sales = pd.read_csv(url)\n",
    "  df_sales['target'] = df_sales.Purchase\n",
    "  df_sales.drop(\n",
    "      columns=['Purchase'],\n",
    "      inplace=True\n",
    "  )\n",
    "\n",
    "  train, test = train_test_split(df_sales, test_size=0.3)\n",
    "  train.to_csv(dataset_train.path + \".csv\" , index=False)\n",
    "  test.to_csv(dataset_test.path + \".csv\" , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\n",
    "      \"pandas\",\n",
    "      \"scikit-learn==1.3.0\"\n",
    "  ], base_image=\"python:3.10\",\n",
    ")\n",
    "def train_blackfriday(\n",
    "  # Use Input[T] to get a metadata-rich handle to the\n",
    "  # input artifact of type `Dataset`.\n",
    "  dataset: Input[Dataset],\n",
    "  model: Output[Model],\n",
    "):\n",
    "  import pickle\n",
    "  import pandas as pd\n",
    "  from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "  data = pd.read_csv(dataset.path + \".csv\")\n",
    "  model_rf = RandomForestRegressor(n_estimators=10)\n",
    "  model_rf.fit(\n",
    "      data.drop(columns=[\"target\"]),\n",
    "      data.target,\n",
    "  )\n",
    "  model.metadata[\"framework\"] = \"scikit-learn\"\n",
    "  file_name = model.path + \".pkl\"\n",
    "  with open(file_name, 'wb') as file:\n",
    "      pickle.dump(model_rf, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install = [\n",
    "      \"pandas\",\n",
    "      \"xgboost==1.7.1\",\n",
    "      \"scikit-learn==1.3.0\"\n",
    "  ], base_image=\"python:3.10\",\n",
    ")\n",
    "def blackfriday_evaluation(\n",
    "  test_set:  Input[Dataset],\n",
    "  rf_blackfriday_model: Input[Model],\n",
    "  # thresholds_dict_str: str,\n",
    "  kpi: Output[Metrics]\n",
    ")-> NamedTuple(\"output\", [(\"deploy\", str)]):\n",
    "#-> NamedTuple(\"Metrics\", [(\"MAE\", float), (\"MSE\", float), (\"R_squared\", float), (\"deploy\", str)])\n",
    "\n",
    "\n",
    "  # from sklearn.ensemble import RandomForestClassifier\n",
    "  import pandas as pd\n",
    "  import logging\n",
    "  import pickle\n",
    "  # from sklearn.metrics import roc_curve, confusion_matrix, accuracy_score\n",
    "  from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "  import numpy as np\n",
    "  import json\n",
    "  import typing\n",
    "\n",
    "\n",
    "\n",
    "  data = pd.read_csv(test_set.path+\".csv\")\n",
    "  file_name = rf_blackfriday_model.path + \".pkl\"\n",
    "  with open(file_name, 'rb') as file:\n",
    "      model = pickle.load(file)\n",
    "\n",
    "  X_test = data.drop(columns=[\"target\"])\n",
    "  y_target = data.target\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  mae = mean_absolute_error(y_target, y_pred)\n",
    "  mse = mean_squared_error(y_target, y_pred)\n",
    "  r2 = r2_score(y_target, y_pred)\n",
    "\n",
    "  kpi.log_metric(\"MAE\", float(mae))\n",
    "  kpi.log_metric(\"MSE\", float(mse))\n",
    "  kpi.log_metric(\"R_squared\", float(r2))\n",
    "\n",
    "  deploy = \"true\"\n",
    "  return (deploy,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "  packages_to_install=[\"google-cloud-aiplatform\", \"scikit-learn==1.3.0\", \"xgboost==1.7.1\", \"kfp\"],\n",
    "  base_image=\"python:3.10\",\n",
    "  output_component_file=\"model_blackfriday_coponent.yml\"\n",
    ")\n",
    "def deploy_blackfriday(\n",
    "  model: Input[Model],\n",
    "  project: str,\n",
    "  region: str,\n",
    "  serving_container_image_uri : str,\n",
    "  vertex_endpoint: Output[Artifact],\n",
    "  vertex_model: Output[Model]\n",
    "):\n",
    "  from google.cloud import aiplatform\n",
    "  aiplatform.init(project=project, location=region)\n",
    "\n",
    "  DISPLAY_NAME  = \"blackfridaysales\"\n",
    "  MODEL_NAME = \"blackfriday-rf\"\n",
    "  ENDPOINT_NAME = \"blackfriday_endpoint\"\n",
    "\n",
    "  def create_endpoint():\n",
    "      endpoints = aiplatform.Endpoint.list(\n",
    "        filter='display_name=\"{}\"'.format(ENDPOINT_NAME),\n",
    "        order_by='create_time desc',\n",
    "        project=project,\n",
    "        location=region,\n",
    "      )\n",
    "      if len(endpoints) > 0:\n",
    "          return endpoints[0]  # most recently created\n",
    "      else:\n",
    "          return aiplatform.Endpoint.create(\n",
    "            display_name=ENDPOINT_NAME, project=project, location=region\n",
    "        )\n",
    "  endpoint = create_endpoint()\n",
    "\n",
    "  #Import a model programmatically\n",
    "  model_upload = aiplatform.Model.upload(\n",
    "      display_name = DISPLAY_NAME,\n",
    "      artifact_uri = model.uri.replace(\"model\", \"\"),\n",
    "      serving_container_image_uri = serving_container_image_uri,\n",
    "      serving_container_health_route=f\"/v1/models/{MODEL_NAME}\",\n",
    "      serving_container_predict_route=f\"/v1/models/{MODEL_NAME}:predict\",\n",
    "      serving_container_environment_variables={\n",
    "      \"MODEL_NAME\": MODEL_NAME,\n",
    "  },\n",
    "  )\n",
    "  model_deploy = model_upload.deploy(\n",
    "      machine_type=\"n1-standard-4\",\n",
    "      endpoint=endpoint,\n",
    "      traffic_split={\"0\": 100},\n",
    "      deployed_model_display_name=DISPLAY_NAME,\n",
    "  )\n",
    "\n",
    "  # Save the resource name to the output params\n",
    "  vertex_model.uri = model_deploy.resource_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "DISPLAY_NAME = 'pipeline-blackfriday-job{}'.format(TIMESTAMP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "  # Default pipeline root. You can override it when submitting the pipeline.\n",
    "  pipeline_root=PIPELINE_ROOT,\n",
    "  # A name for the pipeline. Use to determine the pipeline Context.\n",
    "  name=\"pipeline-blackfriday\",\n",
    ")\n",
    "def pipeline(\n",
    "  url: str = \"https://storage.googleapis.com/randomforest-blackfriday/train.csv\",\n",
    "  project: str = PROJECT_ID,\n",
    "  region: str = REGION,\n",
    "  display_name: str = DISPLAY_NAME,\n",
    "  api_endpoint: str = REGION+\"-aiplatform.googleapis.com\",\n",
    "  serving_container_image_uri: str = \"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-3:latest\"\n",
    "):\n",
    "  data_op = get_blackfriday_data(url=url)\n",
    "  train_model_op = train_blackfriday(dataset = data_op.outputs[\"dataset_train\"])\n",
    "  model_evaluation_op = blackfriday_evaluation(\n",
    "      test_set=data_op.outputs[\"dataset_test\"],\n",
    "      rf_blackfriday_model=train_model_op.outputs[\"model\"],\n",
    "      # thresholds_dict_str = thresholds_dict_str, # I deploy the model anly if the model performance is above the threshold\n",
    "  )\n",
    "\n",
    "  with dsl.Condition(\n",
    "      model_evaluation_op.outputs[\"deploy\"]==\"true\",\n",
    "      name=\"deploy-blackfriday\",\n",
    "  ):\n",
    "      deploy_model_op = deploy_blackfriday(\n",
    "        model=train_model_op.outputs['model'],\n",
    "        project=project,\n",
    "        region=region,\n",
    "        serving_container_image_uri = serving_container_image_uri,\n",
    "      )\n",
    "\n",
    "# pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline,\n",
    "    package_path='ml_blackfriday.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline = pipeline_jobs.PipelineJob(\n",
    "  display_name=\"blackfriday-pipeline\",\n",
    "  template_path=\"ml_blackfriday.json\",\n",
    "  enable_caching=True,\n",
    "  location=REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_pipeline.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import pandas as pd\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "data_path = 'gs://gcp-ml-specialization-demo-black-friday-sales/pipeline_root_blackfridaysales/296237320026/pipeline-blackfriday-20240308121252/get-blackfriday-data_-3824834746941177856/dataset_test.csv'\n",
    "with fs.open(data_path, 'rb') as f:\n",
    "    test_df = pd.read_csv(f, nrows=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instances\n",
    "instances = test_df.drop(columns='target').values.tolist()\n",
    "\n",
    "instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = !(gcloud ai endpoints list --region=$REGION \\\n",
    "              --format='value(ENDPOINT_ID)'\\\n",
    "              --filter=display_name=$ENDPOINT_NAME \\\n",
    "              --sort-by=creationTimeStamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDPOINT_ID = '474208369943511040' # the most recent endpoint\n",
    "\n",
    "\n",
    "# test = instances[0]\n",
    "# print(test)\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
    "prediction = endpoint.predict(instances)\n",
    "\n",
    "prediction.predictions"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
